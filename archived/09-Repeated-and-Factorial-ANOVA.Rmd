---
title: "Repeated measures and factorial ANOVA"
subtitle: "PSY9219M - Research Methods and Skills"
author: "Dr Matt Craddock"
date: "27/11/2018"
output:
  xaringan::moon_reader:
    css: ["default", "default-fonts", "css/my-theme.css"]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
      beforeInit: "js/macros.js"
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
library(tidyverse)
library(afex)
library(emmeans)
library(copula)
library(car)
crime <- read_csv("data/crime.csv")
set.seed(104)
within_cop <- mvdc(normalCopula(param = 0.5, dim = 3),
                   c("norm", "norm", "norm"),
                   paramMargins = list(list(mean = 10, sd = 2),
                                       list(mean = 9, sd = 2),
                                       list(mean = 8, sd = 2)))
test_within <- rMvdc(50, within_cop)
```

# Multiple linear regression

.pull-left[
Multiple regression is what we need with multiple predictors.
```{r multi-reg-four, eval = FALSE}
a <- 2    # Our intercept term
b1 <- 0.65  # Our first regression coefficient
X1 <- rnorm(1000, 6, 1) # Our first predictor 
b2 <- -0.8 # Our second regression coefficient
X2 <- rnorm(1000, 3, 1) # Our second predictor
err <- rnorm(1000, 0, 1) # Our error term
y <- a + b1 * X1 + b2 * X2 + err # Our response variable
```
]
.pull-right[
A simple regression has one predictor...
```{r eval = FALSE}
lm(y ~ X1)
```

and adding predictors is easy - we use the **+** symbol!
```{r eval = FALSE}
lm(y ~ X1 + X2)
```


]

---
# Comparing three or more means with ANOVA

The *t.test()* can only handle two groups. 

When we have three or more groups, we need to use a One-Way Analysis of Variance (ANOVA).

```{r hypothetical-norms, echo = FALSE, fig.height = 5}
tibble(iq = 0:200,
       group_a = dnorm(iq, mean = 100, sd = 15),
       group_b = dnorm(iq, mean = 110, sd = 15),
       group_c = dnorm(iq, mean = 70, sd = 15)) %>%
  gather(group, density, -iq) %>%
  ggplot(aes(x = iq, y = density)) +
  geom_line(aes(group = group, linetype = group)) +
  theme_classic() 
```

---
# Mean squared error and the F-ratio

Finally, we divide our sums of squares - $SS_m$ and $SS_r$ by $df_m$ and $df_r$ respectively, giving us the mean squared error of the model - $MS_m$ - and mean squared error of the residuals - $MS_r$.

$$MS_m = \frac{SS_m}{df_m}$$

$$MS_r = \frac{SS_r}{df_r}$$

The ratio of these two quantities is the *F-ratio*.

$$F = \frac{MS_m}{MS_r}$$

In English, the F-ratio is the ratio of the variability explained by the model to variability unexplained by the model. So, higher is better.

---
class: inverse, middle, center
# Comparing multiple means with dependent data

---
# Within-subjects ANOVA

When the assumption of *independence* is violated - i.e. participants contribute more than one data point, and contribute to more than one design *cell* - we need to use a *within-subjects* or *repeated-measures* ANOVA.

Examples might include heart rate before, during, and after exercise.

# A worked example
Our researcher from last week wanted to examine the effect of noisy environments on test performance. She recruited 150 participants and splits them into three groups who took the test with no noise, reasonably quiet noise, or loud noise.

One problem here is the possibility that participants in each group just had different levels of ability. To get round this, she decides to get each participant to sit three tests, each under different levels of noise. Thus, any differences attributed to  noise can't be due to test-taking ability.


---
# Within-subjects ANOVA

.pull-left[
Last time we simulated the data like this:
```{r sim-dat-old}
noise_test <- 
  gather(tibble(none = rnorm(50, 7.5, 1.5),
                quiet = rnorm(50, 6.5, 1.5),
                loud = rnorm(50, 5, 1.5)),
         noise, test_score) %>%
  mutate(participant = 1:150)
```

One row per observation, which meant one row per participant.
]
.pull-right[
```{r}
head(noise_test)
```
]

---
# Within-subjects ANOVA

.pull-left[
But this time, it's the same participants in each condition. We need to change the participant identifier to reflect that:
```{r}
noise_test_within <- 
  gather(tibble(none = rnorm(50, 7.5, 1.5),
                quiet = rnorm(50, 6.5, 1.5),
                loud = rnorm(50, 5, 1.5)),
         noise, test_score) %>%
  mutate(participant = rep(1:50, 3))
```
There's still one row per observation, but now there are three rows per participant.
]
.pull-right[
```{r}
noise_test_within %>% arrange(participant)
```
]

---
# The mean as a model (again)

The mean as a basic model still counts here. This time we also need to take account of the fact that each person contributes multiple datapoints.

But we still start with the same model using the grand mean.

.pull-left[
```{r echo = FALSE,fig.height=5}
ggplot(noise_test_within,
       aes(x = participant,
           y = test_score,
           colour = noise)) +
  geom_point() + 
  geom_hline(aes(yintercept = mean(test_score))) + 
  theme_classic() + 
  facet_wrap(~noise)
```
]

.pull-right[
The grand mean test score is `r round(mean(noise_test$test_score), 2)`, shown by the black line.

The total variability in our data is the sum of the squared differences from the grand mean - the Total Sum of Squares, $SS_t$.
]

---
# The group means as a model

Just as we did last time, we also calculate the Model sum of squares - $SS_m$.

.pull-left[
```{r echo = FALSE,fig.height=5, warning = FALSE}
noise_test_within %>%
  group_by(noise) %>%
  mutate(group_mean = mean(test_score)) %>%
ggplot(aes(x = participant,
           y = test_score,
           colour = noise)) +
  geom_point(size = 1.5) + 
  geom_hline(aes(yintercept = mean(test_score))) + 
  stat_summary(fun.y = mean,
               geom = "hline",
               aes(yintercept = group_mean),
               size = 2) +
  geom_hline(aes(yintercept = mean(group_mean)), size = 2) +
  theme_classic() + 
  facet_wrap(~noise)
```
]

.pull-right[
Our Model Sum of Squares - $SS_m$ - is the sum of the squared differences of each group's mean from the *grand mean*.

The group means are shown here using coloured lines.

This is just the same as it is for a between-subjects ANOVA.

But the next step is different!
]

---
# Within-subject variability.

Last time we calculated the Residual sum of squares - $SS_r$ - the sum of the squared differences of each individual's score from each group's grand mean. We still need that, but we need to calculate it differently.

But here, we have the same people in each group, and we're interested in how the scores vary *within each person*, not *within each group*.

.pull-left[
```{r echo = FALSE,fig.height=5, warning = FALSE}
noise_test_within %>%
  filter(participant <= 10) %>%
  mutate(participant = as.factor(participant)) %>%
ggplot(aes(x = participant,
           y = test_score)) +
  geom_point(size = 2, aes(colour = noise)) + 
  stat_summary(fun.y = mean,
               geom = "point",
               shape = "triangle",
               size = 3,
               alpha = 0.6) + 
  theme_classic() 
```
]
.pull-right[
We need the within-participant sum of squares - $SS_w$. This is the sum of squared differences of each participant's scores from their individual mean.

Each participant's mean is marked using a triangle, while scores from individual conditions are marked with points (I only show 10 participants for clarity!).
]
---
# The leftovers, the mean squares, and the F-ratio

Finally, we can calculate the Residual sum of squares - $SS_r$ by subtracting the model sum of squares - $SS_m$ - from the within-subjects sum of squares - $SS_w$.

We then calculate the Model Mean Square Error - $MS_m$ - and Residual Mean Square Error - $MS_r$ - the same way as last time, using the degrees of freedom - 

$$MS_m = \frac{SS_m}{df_m}$$

$$MS_r = \frac{SS_r}{df_r}$$

And we calculate the *F-ratio* in the same way as last time.

$$F = \frac{MS_m}{MS_r}$$

---
# Between- versus within-subject ANOVA

.large[
1.  The underlying computations are mostly the same, but differ in how they treat the variability

2.  Within-subject designs use within-subject variability
    - Within-subject variability is often much lower than between-subject variability
    - People function as their own controls!

3.  Since the variance within-subjects is generally lower than between-subjects, within-subject designs typically have more *statistical power* i.e. are more *sensitive*.

4.  However, there is a risk of *order* or *practice* effects with within-subject designs.
]

---
class: inverse, center, middle
# How to run a one-way within-subjects ANOVA 

---
# Within-subjects ANOVA with **afex**

Just like last week, we can use **aov_ez()** from the **afex** package.

Instead of passing a parameter called *between*, we pass one called *within*.

```{r }
noise_within_aov <- 
  aov_ez(dv = "test_score",
       id = "participant",
       within = "noise",
       data = noise_test_within)
noise_within_aov
```

---
# Within-subjects ANOVA

We can follow up the significant effect in the same way as last time:
```{r}
emmeans(noise_within_aov, ~noise)
```

```{r}
pairs(emmeans(noise_within_aov, ~noise))
```

---
# The sphericity assumption

*Sphericity* is the equivalent to the homogeneity of variance assumption, but only applies when there are three or more levels of a repeated measures factor.

Note down at the bottom here, the output says "Sphericity correction method: GG"

GG stands for Greenhouse-Geisser.

**afex** applies GG correction *by default*.

```{r}
noise_within_aov
```

---
# The sphericity assumption

```{r}
summary(noise_within_aov)
```

---
# Reporting the results 

```{r}
noise_within_aov
```

"There was a significant effect of noise level on test scores, [F(1.80, 88.25) = 43.88, p < .001]." Then report the results of any post-hoc tests.

---
class: inverse, middle, center
# Comparing multiple means with multiple categorical predictors

---
# Factorial ANOVA

Sometimes (in fact, often) we have more than one independent variable. For example, in the fear of crime dataset we have *sex* and *victim_crime* as categorical variables with two levels each.

```{r}
head(crime)
```

---
# Main effects and interactions 

We can test the effects of multiple independent variables simultaneously using factorial ANOVA. These are called the *main effects*.

But frequently, what we really want to know is whether the effects of one variable depend on the effects of another variable. These are *interactions*.

```{r}
head(crime)
```

---
# Back to our noise example

.pull-left[
Our researcher now wonders whether the level of noise matters more for tests that are hard compared to tests that are relatively easy.

So she runs the study again, with the same three noise conditions, but now splits the participants into two more conditions. Half of the participants take an easy test, while the other half take a hard test.

]

.pull-right[
```{r more-sim-dat}
noise_test <- 
  gather(tibble(none_hard = rnorm(40, 7, 1.5),
                quiet_hard = rnorm(40, 5.5, 1.5),
                loud_hard = rnorm(40, 5, 1.5),
                none_easy = rnorm(40, 8.5, 1.5),
                quiet_easy = rnorm(40, 8.5, 1.5),
                loud_easy = rnorm(40, 8, 1.5)),
         noise, test_score) %>%
  mutate(participant = 1:240)
```
]

---
# What design does the researcher have?

.large[
The resulting experiment has **two** independent, categorical variables, and thus two *factors*.

The factor "test difficulty" two levels - "easy" and "hard".

The factor "noise" has three levels - "none", "quiet", and "loud".

Both of these factors are *between-subjects*, since there are different participants at each level of each factor.

This is a Two-Way, $2 \times 3$ between-subjects ANOVA.
]


---
# The structure of the data

In this simulated dataset, we have one column tagging which combination of the two factors each datapoint is from; a column with the dependent variable, "test_score", and a column with a participant identifier.

```{r}
head(noise_test)
```

---
# The structure of the data

.pull-left[
We need to have **one column per factor**.

I use the **separate()** function to split "noise" into "noise" and "difficulty".

Note that there's *one row per observation*, which in this case is the same as one row per participant.

```{r}
noise_test <-
  noise_test %>%
  separate(noise,
           into = c("noise",
                    "difficulty"),
           sep = "_")
```
]

.pull-right[
```{r}
head(noise_test)
```
]

---
# A worked example

.pull-left[
As always, a good way to start is to **plot your data**.
```{r echo = FALSE}
noise_test$noise <- ordered(noise_test$noise, levels = c("none", "quiet", "loud"))
```
```{r gg-inter, fig.height = 5, fig.show = "hide"}
ggplot(noise_test,
       aes(x = noise,
           y = test_score,
           shape = difficulty)) + 
  stat_summary(fun.data = mean_cl_normal) +
  theme_classic()
```

On this plot, it looks like there may have been an *interaction* between noise and test difficulty!

]
.pull-right[
![](`r knitr::fig_chunk("gg-inter", "png")`)
]

---
# A worked example

Before running an ANOVA, let's check the assumption of homogeneity of variance.

As before, we use **leveneTest()** from the **car** package.

```{r}
leveneTest(test_score ~ noise * difficulty, data = noise_test)
```

Note that since we have *two* factors, we need to do something a little different: we need to tell R there's an interaction using an asterisk -** * **- between the two.

---
# A worked example

Now, let's run our two-way, $2 \times 3$ ANOVA with **aov_ez()** from the **afex** package.

Since both variables in this example are between-subjects, we just pass both column names to the *between* argument of **aov_ez()**.

```{r two-way-anova}
noise_aov <- 
  aov_ez(dv = "test_score",
         between = c("noise",
                     "difficulty"),
         id = "participant",
         data = noise_test)
```

---
# A worked example

Let's look at the results.
```{r}
noise_aov
```

Everything is significant! Where should we start?

---
# A worked example

.pull-left[

Here's a similar plot to the one I produced earlier, but using **afex_plot()** instead of **ggplot()**.

```{r afx-plot, fig.show = "hide", fig.height = 5}
afex_plot(noise_aov,
          x = "noise",
          trace = "difficulty", 
          error = "between") +
  theme_classic()
```

It seems pretty obvious from this plot that there's an effect of noise when the test is hard, but not so much when the test is easy. This is an interaction effect.
]
.pull-right[
![](`r knitr::fig_chunk("afx-plot", "png")`)
]

---
# Post-hoc tests

We need to follow up a significant interaction to work out, statistically, what is driving the interaction.

One way to do this is with post-hoc tests. With post-hoc tests, we compare every possible pair of means to each other using t-tests.

First let's get all the means using **emmeans()**.

```{r}
all_means <- emmeans(noise_aov,
                     ~noise * difficulty)
all_means
```

---
# Post-hoc tests

Then we use the **pairs()** function to test them all against each other.
```{r}
pairs(all_means)
```

---
# Post-hoc tests

.large[
1. Should only be used following a *significant* interaction.

2. Leads to a lot of comparisons - N(N-1) / 2, where N is the number of means. So it's **extremely important** to correct for multiple comparisons!
    - Numerous methods exist; fortunately, **emmeans()** and **pairs()** handle this for us using Tukey's Honestly Significant Difference (HSD). 

3. Can be difficult to interpret, especially with more than two factors. If in doubt, *look at the plots*.
]

---
# Simple effects

An alternative way is with *simple effects*. We can effectively run separate analyses at different levels of one of the factors. Here I look at the means for each level of noise separately for the two levels of difficulty.

```{r}
emmeans(noise_aov,
        "noise",
        by = "difficulty")
```

---
# Simple effects

Now we run post-hoc tests separately within each level of difficulty.

```{r}
pairs(emmeans(noise_aov,
        "noise",
        by = "difficulty"))
```

---
# Simple effects
.large[
1.  Often easier to interpret (especially when there are more than two factors).

2.  Fewer comparisons so less stringent correction for multiple comparison, and higher power to detect differences.

3.  Not always obvious which factor to separate on. Sometimes it's easier to interpret one way than the other! Again, *use plots as a guide*.
]

---
# What about the main effects?

I skipped straight to the interaction earlier. Why?

```{r}
noise_aov
```

There are significant main effects of *noise* and *difficulty*.



---
# The main effect of noise

There's a significant main effect of noise. Let's look at the plot.

```{r fig.height = 5}
afex_plot(noise_aov, ~noise) + theme_bw()
```


---
# The main effect of noise

As noise increases, test performance goes down.
```{r}
emmeans(noise_aov, ~noise)
```

---
# The main effect of difficulty

Let's look at the plot for difficulty.

```{r fig.height = 5}
afex_plot(noise_aov, ~difficulty) + theme_bw()
```

---
# The main effect of difficulty

Test performance is much higher when the test is easy than when it's hard.

```{r}
emmeans(noise_aov, ~difficulty)
```

---
# Are these effects meaningful?

.pull-left[
```{r inter-plot-again, fig.height = 5, fig.show = "hide"}
afex_plot(noise_aov,
          ~noise,
          trace = "difficulty") +
  theme_bw()
```

There is clearly no significant effect of noise when the test is easy, so the main effect of noise is uninterpretable.

But there is *always* an effect of test difficulty, so the main effect of test difficulty *is* interpretable.

**Main effects are not always interpretable in the presence of an interaction!**
]
.pull-right[
![](`r knitr::fig_chunk("inter-plot-again", "png")`)
]

---
class: inverse, middle, center
# Mixed designs

---
# Mixed ANOVA

.pull-left[
Sometimes some of our variables are within-subjects and some are between subjects. 

We can handle this situation using *mixed ANOVA*. 

Suppose that different people were in the easy and hard test difficulty groups, but each one did repeated tests under no noise, quiet noise, and loud noise.
]
.pull-right[
```{r}
noise_test_mixed <- 
  gather(tibble(none_hard = rnorm(40, 7, 1.5),
                quiet_hard = rnorm(40, 5.5, 1.5),
                loud_hard = rnorm(40, 5, 1.5),
                none_easy = rnorm(40, 8.5, 1.5),
                quiet_easy = rnorm(40, 8.5, 1.5),
                loud_easy = rnorm(40, 8, 1.5)),
         noise, test_score) %>%
  mutate(participant = c(rep(1:40, 3), rep(41:80, 3)))
```
]

---
# The structure of the data


```{r echo = FALSE}
noise_test_mixed <- 
  noise_test_mixed %>%
  separate(noise,
           into = c("noise", "difficulty"),
           sep = "_") 
```
.pull-left[
Since each participant takes part in all three noise conditions, there are three rows per participant.
```{r}
noise_test_mixed %>%
  arrange(participant)
```
]
.pull-right[
But since difficulty is between-subjects, the participant ID numbers differ across easy and hard difficulties.
```{r}
noise_test_mixed %>%
  arrange(difficulty, participant)
```
]

---
# Mixed ANOVA with **afex**

We just pass the between-subjects variable to the "between" argument, and the within-subjects variable to the "within" argument. Everything else works the same way! 
```{r message = FALSE}
aov_ez(dv = "test_score",
       within = "noise",
       between = "difficulty",
       id = "participant",
       data = noise_test_mixed)
```

---
class: inverse, middle, center
# Reporting factorial ANOVA results

---
# Reporting factorial ANOVAs

.large[
Make sure that somewhere in your text is a description of which type of ANOVA you are running, and exactly what factors are involved. A couple of examples:

1.  We conducted a two-way repeated measures ANOVA with the factors Noise (None, Quiet, or Loud) and Difficulty (Easy or Hard).

2.  We conducted a $2 \times 3$ mixed ANOVA. The between-subjects factor was Difficulty (Easy or Hard), while Noise was a repeated-measures factor (None, Quiet, or Loud)
]

---
# Reporting factorial ANOVAs

```{r echo = FALSE}
noise_aov
```

There was a significant main effect of noise [F(2, 234) = 6.29, p = .002], with  better test performance when there was no noise (mean = 7.71) compared to when there was loud noise (mean = 6.86, p = .002). No other comparisons were significant (all p > .05).

---
# Reporting factorial ANOVAs

```{r echo = FALSE}
noise_aov
```

There was also a significant main effect of Difficulty [F(1, 234) = 185.84, p < .001], with better test performance when the test was easy (mean = 8.61) compared to when the test was hard (mean = 5.91, p < .001).

---
# Reporting factorial ANOVAs

```{r echo = FALSE}
noise_aov
```

Finally, there was also a significant interaction between Noise and Difficulty [F(2, 234) = 6.35, p = .002], see Figure X. Simple main effects analysis, corrected for multiple comparisons using Tukey's HSD, found that in the Easy difficulty condition, there were no significant differences between any of the Noise conditions (all p > .05). However, in the Hard condition, test performance was significantly better when there was no noise (mean = 6.82) compared to when there was either quiet (5.80; p = .009) or loud noise (5.11, p < .001). Quiet and loud noise did not significantly differ (p = .1).

---
class: title-slide-final, middle, inverse
background-image: url('images/University of Lincoln_logo_General White Landscape.png')
background-size: 500px
background-position: 50% 10%

