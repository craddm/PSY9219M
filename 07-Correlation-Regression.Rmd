---
title: "Correlation and regression"
subtitle: "PSY9219M - Research Methods and Skills"
author: "Dr Matt Craddock"
date: "13/11/2018"
output:
  xaringan::moon_reader:
    css: ["default", "default-fonts", "css/my-theme.css"]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
      beforeInit: "js/macros.js"
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
library(tidyverse)
library(sjPlot)
library(copula)
library(stargazer)
crime <- read_csv("data/crime.csv")
```

# Null Hypothesis Significance Testing (NHST)

Think back to our previous questions:

1.  Do men and women differ in terms of their fear of crime?

2.  Are people who have been a victim of crime more fearful of crime?

The basis of NHST is to phrase these questions as: 

If there is only one population, how likely is it that our two samples have values this different from each other?

We answer this question with *test statistics* and *p-values*.
---
# The normal distribution
```{r hypothetical-norm-two, echo = FALSE, fig.height = 5}
tibble(iq = 0:200,
       thin = dnorm(iq, mean = 100, sd = 15),
       fat = dnorm(iq, mean = 120, sd = 30)) %>%
  gather(dist_type, density, -iq) %>%
  ggplot(aes(x = iq, y = density)) +
  geom_line(aes(group = dist_type, linetype = dist_type)) +
  theme_classic() 
```

---
# Performing *t*-tests in R

.pull-left[
The tilde (~) symbol in R usually means "modelled by" 

`FoC ~ victim_crime` means `FoC modelled by victim_crime`.

`data = crime` tells R to look in the `crime` data frame for the data.

`paired = FALSE` tells R that this is an *independent samples* test. 

]
.pull-right[
```{r inital-t}
t.test(FoC ~ victim_crime,
       data = crime,
       paired = FALSE)
```
]

---
class: inverse, middle, center
# The mean and the variance

---
# The mean and the variance

.pull-left[
```{r hypothetical-norm, echo = FALSE, fig.height = 5}
tibble(variable = seq(0, 6, by = 0.1),
       density = dnorm(variable, mean = 3, sd = 1)) %>%
  ggplot(aes(x = variable, y = density)) +
  geom_line() +
  theme_classic() +
  geom_vline(xintercept = 3, linetype = "dashed") +
  geom_vline(xintercept = c(2, 4), linetype = "dotted") +
  annotate(geom = "text", x = 1, y = 0.425, label = "Mean = 3") +
  annotate(geom = "text", x = 1, y = 0.40, label = "Standard deviation = 1")
```
]
.pull-right[
The mean - $\mu$ -  is a measure of *central tendency*.

The standard deviation - $\sigma$ - is a measure of the *spread* of the data - i.e. 
]


---
# The mean and the variance 

.pull-left[
```{r quick-histo, fig.height = 5, fig.show = "hide"}
ggplot(crime, aes(x = E)) +
  geom_histogram(binwidth = 0.2) +
  geom_vline(aes(xintercept = mean(E))) +
  theme_classic()
```

```{r}
mean(crime$E)
sd(crime$E)
```
]
.pull-right[
![](`r knitr::fig_chunk("quick-histo",".png")`)
]
---
# The mean and the variance 

.pull-left[
As you can see, the values don't lie directly on the mean, but are spread around it.

To quantify how much the values vary from the mean, we can calculate the *variance*.

Here's the scary looking formula for the variance:

$$  \sigma^2 = \frac{\sum(x - \bar{x})^2}{N - 1} $$

And here's the not-so-scary R function:

```{r eval = FALSE}
var(x)
```

]
.pull-right[
```{r fig.height = 5, echo = FALSE}
tmp_dat <- data.frame(participant = factor(1:10),
                      E = crime$E[1:10])
ggplot(tmp_dat,
       aes(x = participant, y = E)) + 
  geom_point() + 
  geom_hline(aes(yintercept = mean(E))) + 
  geom_segment(aes(xend = participant,
                   yend = mean(E)),
               linetype = "dashed") +
  geom_label(aes(label = E), nudge_y = 0.15) +
  theme_classic()
```

]

---
# The mean and the variance 

.pull-left[
Let's break down the top part of the formula:
$$ \sum{(x - \bar{x})^2} $$

This is the *sum* of the *squared differences* from the mean.

The first step is to subtract the mean of all values from the values themselves: $x - \bar{x}$

In R code this is:
```{r eval = FALSE}
x - mean(x)
```
]
.pull-right[
```{r fig.height = 5, echo = FALSE}
tmp_dat <- data.frame(participant = factor(1:10),
                      E = (crime$E[1:10] - mean(crime$E[1:10])))
ggplot(tmp_dat,
       aes(x = participant, y = E)) + 
  geom_point() + 
  geom_segment(aes(xend = participant,
                   yend = 0),
               linetype = "dashed") +
  theme_classic() +
  geom_hline(yintercept = 0) + 
  geom_label(aes(label = round(E, digits = 2)), nudge_y = 0.15) 
```
]

---
# The mean and the variance

.pull-left[
The next step is to *square* those differences to get $(x - \bar{x})^2$.

This has a couple of consequences:

1.  Negative values become positive. 

2.  Values that are further away from the mean often get even further away.

This prevents "errors" from cancelling out, and effectively penalises values that are far away from the mean.
]
.pull-right[
```{r fig.height = 5, echo = FALSE}
tmp_dat <- data.frame(participant = factor(1:10),
                      E = (crime$E[1:10] - mean(crime$E[1:10])) ^2)
ggplot(tmp_dat,
       aes(x = participant, y = E)) + 
  geom_point() + 
  geom_segment(aes(xend = participant,
                   yend = 0),
               linetype = "dashed") +
  theme_classic() +
  geom_hline(yintercept = 0) + 
  geom_label(aes(label = round(E, digits = 2)), nudge_y = 0.1) 
```

]


---
# The mean and the variance

.pull-left[
Next, we add those numbers together: 
$$\sum{(x - \bar{x})^2}$$

In R code, this is:
```{r eval = FALSE}
sum((x - mean(x))^2)
```

```{r}
sum((crime$E[1:10] - mean(crime$E[1:10]))^2)
```

]
.pull-right[
```{r fig.height = 5, echo = FALSE}
tmp_dat <- data.frame(participant = factor(1:10),
                      E = (crime$E[1:10] - mean(crime$E[1:10])) ^2)
ggplot(tmp_dat,
       aes(x = participant, y = E)) + 
  geom_point() + 
  geom_segment(aes(xend = participant,
                   yend = 0),
               linetype = "dashed") +
  theme_classic() +
  geom_hline(yintercept = 0) + 
  geom_label(aes(label = round(E, digits = 2)), nudge_y = 0.1) 
```
]

---
# The mean and the variance

.pull-left[
The final step is to take the average: 
$$\frac{\sum{(x - \bar{x})^2}}{N - 1}$$

In R code, this is:
```{r eval = FALSE}
sum((x - mean(x))^2) / (length(x) - 1)
```
```{r}
sum((crime$E[1:10] - mean(crime$E[1:10]))^2) / (length(crime$E[1:10]) - 1)
```

...or, for short:
```{r}
var(crime$E[1:10])
```

]
.pull-right[
```{r fig.height = 5, echo = FALSE}
tmp_dat <- data.frame(participant = factor(1:10),
                      E = (crime$E[1:10] - mean(crime$E[1:10])) ^2)
ggplot(tmp_dat,
       aes(x = participant, y = E)) + 
  geom_point() + 
  geom_segment(aes(xend = participant,
                   yend = 0),
               linetype = "dashed") +
  theme_classic() +
  geom_hline(yintercept = 0) + 
  geom_label(aes(label = round(E, digits = 2)), nudge_y = 0.1) 
```
]

---
class: inverse, middle, center
# Correlation  

---
# Correlation

How related are the variables in the Fear of Crime dataset?

```{r}
head(crime)
```

---
# Correlation

.pull-left[
What happens to Fear of Crime as Emotionality increases?
```{r basic-scatter, fig.height = 5, fig.show = "hide"}
ggplot(crime,
       aes(x = E,
           y = FoC)) + 
  geom_jitter() + 
  theme_classic() +
  labs(x = "Emotionality",
       y = "Fear of Crime") 
```
]
.pull-right[
![](`r knitr::fig_chunk("basic-scatter", ".png")`)
]

---
# Correlation

.pull-left[

There is a positive relationship between Emotionality and Fear of Crime.

```{r basic-scatter-lm, fig.height = 5, fig.show = "hide"}
ggplot(crime,
       aes(x = E,
           y = FoC)) + 
  geom_jitter() + 
  theme_classic() +
  labs(x = "Emotionality",
       y = "Fear of Crime") +
  stat_smooth(method = "lm", se = FALSE)
```
]
.pull-right[
![](`r knitr::fig_chunk("basic-scatter-lm", ".png")`)
]


---
# Different relationships

```{r corr-dir, fig.width = 12, fig.height = 6, echo = FALSE}
pos_corr <- matrix(0.9, 2, 2)
diag(pos_corr) <- 1
neg_corr <- matrix(-.9, 2, 2)
diag(neg_corr) <- 1
no_corr <- matrix(0, 2, 2)
diag(no_corr) <- 1
corr_examps <- data.frame(rbind(MASS::mvrnorm(200, rep(0, 2), Sigma = pos_corr),
                          MASS::mvrnorm(200, rep(0, 2), Sigma = neg_corr),
                          MASS::mvrnorm(200, rep(0, 2), Sigma = no_corr)),
                          direction = rep(c("positive", "negative", "none"),
                                          each = 200))
names(corr_examps) <- c("X", "Y", "direction")
ggplot(corr_examps, aes(x = X, y = Y)) + 
  geom_point() +
  facet_wrap(~direction) +
  theme_classic() +
  theme(text = element_text(size = 26))
```

---
# Correlation and covariance

.pull-left[
Correlation measures the strength and direction of an association between two continuous variables.

But how do we quantify it? We need to look at how the variables *vary* together.

Let's look at some of the statistics of the Emotionality variable.

The plot shows the Emotionality values of the first ten participants.

The line across the middle is the mean of those values - `r mean(crime$E[1:10])`.
]
.pull-right[
```{r emo-plot, fig.height = 5, echo = FALSE}
tmp_dat <- data.frame(participant = factor(1:10),
                      E = crime$E[1:10])
ggplot(tmp_dat,
       aes(x = participant, y = E)) + 
  geom_point() + 
  geom_hline(aes(yintercept = mean(E))) + 
  geom_segment(aes(xend = participant,
                   yend = mean(E)),
               linetype = "dashed") +
  geom_label(aes(label = E), nudge_y = 0.15) +
  theme_classic()
```

]

---
# Correlation and covariance

.pull-left[
Now let's look at the same plot for Fear of Crime (FoC).

Again, the these points and labels are individual ratings of Fear of Crime.

The line across the middle shows the mean, which is `r mean(crime$FoC[1:10])`.

]

.pull-right[
```{r foc-plot, fig.height = 5, echo = FALSE}
tmp_dat <- data.frame(participant = factor(1:10),
                      FoC = crime$FoC[1:10])
ggplot(tmp_dat,
       aes(x = participant, y = FoC)) + 
  geom_point() + 
  geom_hline(aes(yintercept = mean(FoC))) + 
  geom_segment(aes(xend = participant,
                   yend = mean(FoC)),
               linetype = "dashed") +
  geom_label(aes(label = FoC), nudge_y = 0.15) +
  theme_classic()
```

]

---
# Correlation and covariance
.pull-left[

Now let's look at these previous two plots as differences from their respective means.

What we want to now is to what extent the values *vary together*. I.e. as one goes up, does the other? 

This is *covariance*.

Here's the scary formula:

$$ cov(x, y) = \frac{\sum((x - \bar{x})(y - \bar{y}))}{N - 1} $$

Here's the not-so-scary R function:

```{r eval = FALSE}
cov(x, y)
```

]
.pull-right[
```{r fig.height = 6, fig.width = 8, echo = FALSE}
tmp_dat <- data.frame(participant = rep(factor(1:10), 2),
                      y = c(crime$FoC[1:10] - mean(crime$FoC[1:10]),
                            crime$E[1:10] - mean(crime$E[1:10])),
                      var = rep(c("Fear of crime",
                                  "Emotionality"),
                                each = 10))
ggplot(tmp_dat,
       aes(x = participant, y = y)) + 
  geom_point() + 
  geom_hline(aes(yintercept = mean(y))) + 
  geom_segment(aes(xend = participant,
                   yend = mean(y)),
               linetype = "dashed") +
  geom_label(aes(label = y),
             nudge_x = -0.4) +
  theme_classic() + 
  facet_grid(var~.) +
  theme(text = element_text(size = 16))
```
]

---
# Correlation and covariance

.pull-left[
Covariance gives us a measure of how much two variables vary together. 

But the numbers it gives us can be hard to interpret when the variables are on very different scales.

So we rescale the covariance using the standard deviations of each variable.

$$ corr(x, y) = r = \frac{cov(x, y)}{\sigma^x\sigma^y} $$

This gives us the *correlation coefficient*, or *r*.

```{r}
cor(crime$E, crime$FoC)
```

]
.pull-right[
```{r fig.height=5, echo = FALSE}
ggplot(crime,
       aes(x = E,
           y = FoC)) + 
  geom_jitter() + 
  theme_classic() +
  labs(x = "Emotionality",
       y = "Fear of Crime") +
  stat_smooth(method = "lm", se = FALSE)
```
]

---
# Pearson's product-moment correlation

The **cor.test()** function can be used to test the *significance* of a correlation.

```{r cor-test-first}
cor.test(crime$E, crime$FoC,
         method = "pearson")
```

---
# Curved or non-linear relationships

If your data look like this:

.pull-left[
```{r fig.height = 5, echo = FALSE}
X <- rnorm(100)
Y <- X^2 + rnorm(100, 0, 0.25)
qplot(X, Y) + theme_classic()
```
]
.pull-right[
```{r fig.height= 5, echo = FALSE}
X <- rnorm(100)
Y <- X^2 + rnorm(100, 0, 0.25)
qplot(X, -Y) + theme_classic()
```
]
...forget about correlation.

---
# Curved or non-linear relationships

...but if your data look like this:

```{r echo = FALSE, fig.height = 5}
X <- rnorm(100)
Y <- (X ^3 + rnorm(100, 0, .4))  /2
qplot(X, Y) + theme_classic()
```

...there is hope!

---
# Spearman's rank correlation

Spearman's correlation is used to measure *monotonicity*, and is the non-parametric equivalent to Pearson's correlation.

If the data is not already ranks then it is converted to ranks; then the ranks are correlated across variables.

```{r}
cor.test(crime$age,
         crime$E,
         method = "spearman")
```


---
background-image: url(../images/07/chart.png)
background-size: 80%
background-position: 50% 70%
# Correlation is not causation

[https://www.spuriouscorrelations.com](https://www.spuriouscorrelations.com)


---
# Correlation summary

.large[
Correlation coefficients are bound in a range from -1 to 1.

Negative coefficients mean that as one variable increases, the other decreases.

Positive coefficients mean that as one variable increases, the other also increases.
]

```{r corr-dir-again, fig.width = 8, fig.height = 4, echo = FALSE}
pos_corr <- matrix(0.9, 2, 2)
diag(pos_corr) <- 1
neg_corr <- matrix(-.9, 2, 2)
diag(neg_corr) <- 1
no_corr <- matrix(0, 2, 2)
diag(no_corr) <- 1
corr_examps <- data.frame(rbind(MASS::mvrnorm(200, rep(0, 2), Sigma = pos_corr),
                          MASS::mvrnorm(200, rep(0, 2), Sigma = neg_corr),
                          MASS::mvrnorm(200, rep(0, 2), Sigma = no_corr)),
                          direction = rep(c("positive", "negative", "none"),
                                          each = 200))
names(corr_examps) <- c("X", "Y", "direction")
ggplot(corr_examps, aes(x = X, y = Y)) + 
  geom_point() +
  facet_wrap(~direction) +
  theme_classic() +
  theme(text = element_text(size = 16))
```

---
# Reporting a correlation

.large[
Reporting a correlation is pretty straightforward. Only the correlation coefficient and p-value are typically required. e.g. 

"There was a significant positive correlation between emotionality and fear of crime, *r* = .37, *p* < .001."

However, it's best to also specify which type of correlation you used (e.g. Pearson's or Spearman's); and a scatterplot showing the relationship should almost always be shown. Typically, the degrees of freedom or number of observations should also be given, e.g. *r*(299) = .37, *p* < .001, or r = .37, *p* < .001, *N* = 301.

Note that *r* is considered a measure of *effect size*. An *r* of .1 is considered a small effect, while an *r* of .8 is considered a large effect.
]

---
class: inverse, middle, center
# Linear regression

---
# Correlation, regression and prediction

.pull-left[
Correlation quantifies the *strength* and *direction* of an association between two continuous variables.

But what if we want to *predict* the values of one variable from those of another?

For example, as Emotionality increases, *how much* does Fear of Crime change?

```{r basic-lm, fig.show = "hide", fig.height=5}
ggplot(crime, 
       aes(x = E, y = FoC)) +
  geom_point() +
  stat_smooth(method = "lm", se = FALSE) +
  theme_classic() +
  labs(x = "Emotionality",
       y = "Fear of crime")
```
]
.pull-right[
![](`r knitr::fig_chunk("basic-lm", ".png")`)
]

---
# Linear regression

.pull-left[
![](`r knitr::fig_chunk("basic-lm", ".png")`)
]
.pull-right[
The line added to this scatterplot is the *line of best fit*.

A line like this can be described by two parameters - the *intercept* and the *slope*.

The *intercept* is where the line crosses the *y-axis*.

The *slope* is the *steepness* of the line.

Given these parameters, we can predict the value of **y** - the dependent variable -  at each value of **x** - the independent, predictor variable - using the following formula:

$y = a + bX$
]

---
# Line of best fit demo

The line of best fit can be found by adjusting the *intercept* and *slope* to minimise the *sum of squared residuals*.

[Line of best fit demo](https://shinyapps.org/showapp.php?app=https://tellmi.psy.lmu.de/felix/lmfit&by=Felix%20Sch%C3%B6nbrodt&title=Find-a-fit!&shorttitle=Find-a-fit!)

---
# The mean as a model

.pull-left[
First, let's create a linear model that simply finds the *mean* using the **lm()** function.
```{r mean-mod}
intercept_only <- lm(FoC ~ 1, data = crime)
intercept_only
```

Here the Intercept is equal to the *mean* of FoC.

```{r}
mean(crime$FoC)
```
]
.pull-right[
In the formula $y = a + bX$, *a* is the *Intercept*.

So our prediction for the value of *y* is $y = 2.44$.
```{r echo = FALSE, fig.height = 5}
ggplot(crime, aes(x = E, y = FoC)) + 
  geom_point() +
  geom_hline(aes(yintercept = mean(FoC)), colour = "blue") +
  theme_classic()
```
]

---
# Fear of crime predicted by emotionality

.pull-left[
Now let's add Emotionality as a *predictor*.

```{r first-lm}
foc_by_E <- lm(FoC ~ E, data = crime)
foc_by_E
```

We now have two coefficients - one for the intercept, and one for Emotionality.

These are the *intercept* and *slope* of the regression line on the right.
]
.pull-right[
```{r echo = FALSE, fig.height=5}
ggplot(crime, aes(x = E, y = FoC)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) + 
  theme_classic()
```
]

---
# Fear of crime predicted by emotionality
.pull-left[
Again, the regression line is described by the formula $y = a + bX$. So we can fill that out with our model coefficients as follows:

Fear of crime = `r round(foc_by_E$coefficients[1], digits = 2)` + `r round(foc_by_E$coefficients[2], digits = 2)` * $X$

$X$ is the value of the *predictor*.

The *intercept* is now the value of $y$ when the value of the predictor is *zero*.

The coefficient for the predictor is the amount that $y$ increases for each 1 unit increase in the predictor.
]

.pull-right[
```{r echo = FALSE, fig.height=5}
ggplot(crime, aes(x = E, y = FoC)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) + 
  theme_classic()
```
]

---
# Is this a good model of Fear of crime?

We can get more information about our model using the **summary()** function.

```{r}
summary(foc_by_E)
```

---
# Nicer formatting using stargazer!
```{r results="asis"}
stargazer(foc_by_E, single.row = TRUE, type = "html")
```

---
# Fear of crime predicted by emotionality

Let's focus on the coefficients.

```{r}
summary(foc_by_E)$coefficients
```

Estimate is the *coefficient* of each predictor; Std. Error is an estimate of the accuracy of that coefficient.

The significance of each predictor is tested using a t-test; *t value* is thus the t statistic. The *Pr(>|t|)* column is the p-value for that test.

P-values below .05 are considered significant.

Thus, E - Emotionality - is a significant predictor of Fear of Crime. 

Since its coefficient is positive, Fear of Crime increases as Emotionality increases. 

---
# Model-fit

.pull-left[
R-squared ($R^2$) is a measure of model fit. Specifically, it's the proportion of *explained* variance in the data.

We previously looked at the deviation of values from the mean.

After linear regression, we look at deviation of values from the model predictions.

What's left are the *residuals*.
]
.pull-right[
```{r echo = FALSE, fig.height=5}
ggplot(crime[1:20, ], aes(x = E,
                          y = (FoC - predict(foc_by_E)[1:20]))) + 
  geom_point(alpha = 0.5)+
  geom_hline(aes(yintercept = 0)) +
  geom_segment(aes(xend = E, yend = 0), linetype = "dashed") + 
  theme_classic() + 
  labs(y = "FoC")
```
]

---
# Model-fit

.pull-left[
To work out how well our model fits, we first need to know how much *total variation* there is in the data.

For that, we sum the squared differences of the values of the dependent variable from the mean of the dependent variable $y$ - the *total sum of squares*, $SS_t$:

$SS_t = \sum(y - \bar{y})^2$
]
.pull-right[
```{r diff-from-mean, echo = FALSE, fig.height=5}
ggplot(crime[1:20, ], aes(x = E, y = (FoC - mean(FoC)) ^2)) + 
  geom_point(alpha = 0.5)+
  geom_hline(aes(yintercept = 0)) +
  geom_segment(aes(xend = E, yend = 0), linetype = "dashed") + 
  theme_classic() + 
  labs(y = "FoC")
```
]

---
# Model-fit

.pull-left[
We then calculate the sum of the squared differences of the values of the dependent variable ($y$) from the model predictions - the sum of the squared residuals, $SS_r$:

$SS_r = \sum(y - \hat{y})^2$
]
.pull-right[
```{r diff-from-pred, echo = FALSE, fig.height = 5}
ggplot(crime[1:20, ],
       aes(x = E, y = (FoC - predict(foc_by_E)[1:20]) ^2)) + 
  geom_point(alpha = 0.5)+
  geom_hline(aes(yintercept = 0)) +
  geom_segment(aes(xend = E, yend = 0), linetype = "dashed") + 
  theme_classic() + 
  labs(y = "FoC")
```
]

---
# Model-fit

.large[
Finally, we calculate *model sum of squares* - $SS_m$ - as the difference between the *total sum of squares* and the *residual sum of squares*. This tells us, roughly, how much better our model is than just using the *mean*:

$SS_m = SS_t - SS_r$

R-squared ($R^2 $) can then be calculated by dividing the model sum of squares by the total sum of squares:

$R^2 = \frac{SS_m}{SS_t}$

This yields the *percentage of variance explained by the model*. 

This is a long-winded way of saying: Higher $R^2$ means more explained variance, and thus, a better fitting model.

]

---
# Model-fit

Thankfully, R does all these calculations for us!

```{r}
summary(foc_by_E)$r.squared
```

Our simple regression model of the effect of Emotionality on Fear of Crime explained ~ 14% of the variance.

---
# The F-statistic

The last row here shows if the overall model is significantly better than an "intercept only" model. 
```{r}
summary(foc_by_E)
```

---
# Example of reporting a simple regression model

"Simple linear regression was used to investigate the relationship between emotionality and fear of crime. A significant regression equation was found that explained 14% of the variance, $R^2$ = .14, *F*(1, 299) = 47.39, *p* < .001. Fear of crime also increased significantly with increases in Emotionality, $\beta$ = 0.55, t(6.884), p < .001."

---
# Next week

Next week we'll continue with **regression**, looking at multiple predictors.

We'll also begin with **one-way ANOVA** for comparison of multiple means. 

## Reading

Chapter 10 - Comparing Several Means - ANOVA (GLM 1)

---
class: title-slide-final, middle, inverse
background-image: url('images/University of Lincoln_logo_General White Landscape.png')
background-size: 500px
background-position: 50% 10%

---
# Additional support

Maths & Stats Help (AKA MASH) are a service offered by the University, based over in the library.

They offer support to both undergraduate and postgraduate students. You'll find their website at 

[https://guides.library.lincoln.ac.uk/mash](https://guides.library.lincoln.ac.uk/mash)

Note that while their website is mostly about other software, they do support R!

## And a reminder!

My office hours are 1-2 Mondays and Tuesdays. You'll find me at SSB2226. 

