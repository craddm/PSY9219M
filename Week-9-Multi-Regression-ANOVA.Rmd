---
title: "Multiple predictors and multiple means"
date: "8/12/2020"
output:
  xaringan::moon_reader:
    css: ["default", "default-fonts", "xaringan-themer.css", "css/my-theme.css"]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
      beforeInit: "js/macros.js"
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
library(tidyverse)
library(afex)
library(emmeans)
library(ggdag)
library(ggforce)
library(sjPlot)
xaringanExtra::use_webcam()
xaringanExtra::use_tile_view()
xaringanExtra::use_tachyons()
xaringanExtra::use_extra_styles()
knitr::opts_chunk$set(fig.asp = .618,
                      fig.width = 6)
crime <- read_csv("data/crime.csv")
child_data <- read_csv("data/child_data.csv")
```
```{r xaringantheme, include = FALSE, warning = FALSE}
library(xaringanthemer)
style_solarized_light(base_font_size = "20px")
```

# Different correlations

```{r corr-dir, fig.width = 12, fig.height = 6, echo = FALSE}
pos_corr <- matrix(0.9, 2, 2)
diag(pos_corr) <- 1
neg_corr <- matrix(-.9, 2, 2)
diag(neg_corr) <- 1
no_corr <- matrix(0, 2, 2)
diag(no_corr) <- 1
corr_examps <- data.frame(rbind(MASS::mvrnorm(200, rep(0, 2), Sigma = pos_corr),
                          MASS::mvrnorm(200, rep(0, 2), Sigma = neg_corr),
                          MASS::mvrnorm(200, rep(0, 2), Sigma = no_corr)),
                          direction = rep(c("positive", "negative", "none"),
                                          each = 200))
names(corr_examps) <- c("X", "Y", "direction")
ggplot(corr_examps, aes(x = X, y = Y)) + 
  geom_point() +
  facet_wrap(~direction) +
  theme_classic() +
  theme(text = element_text(size = 26))
```

---
# Correlation summary

.large[
Correlation coefficients are bound in a range from -1 to 1.

Negative coefficients mean that as one variable increases, the other decreases.

Positive coefficients mean that as one variable increases, the other also increases.
]

```{r corr-dir-again, fig.width = 8, fig.height = 4, echo = FALSE}
pos_corr <- matrix(0.9, 2, 2)
diag(pos_corr) <- 1
neg_corr <- matrix(-.9, 2, 2)
diag(neg_corr) <- 1
no_corr <- matrix(0, 2, 2)
diag(no_corr) <- 1
corr_examps <- data.frame(rbind(MASS::mvrnorm(200, rep(0, 2), Sigma = pos_corr),
                          MASS::mvrnorm(200, rep(0, 2), Sigma = neg_corr),
                          MASS::mvrnorm(200, rep(0, 2), Sigma = no_corr)),
                          direction = rep(c("positive", "negative", "none"),
                                          each = 200))
names(corr_examps) <- c("X", "Y", "direction")
ggplot(corr_examps, aes(x = X, y = Y)) + 
  geom_point() +
  facet_wrap(~direction) +
  theme_classic() +
  theme(text = element_text(size = 16))
```

---
#Correlation, regression and prediction

.pull-left[
Correlation quantifies the *strength* and *direction* of an association between two continuous variables.

But what if we want to *predict* the values of one variable from those of another?

For example, as Emotionality increases, *how much* does Fear of Crime change?

```{r basic-lm, fig.show = "hide", fig.height=5}
ggplot(crime, 
       aes(x = E, y = FoC)) +
  geom_point() +
  stat_smooth(method = "lm", se = FALSE) +
  theme_classic() +
  labs(x = "Emotionality",
       y = "Fear of crime")
```
]
.pull-right[
![](`r knitr::fig_chunk("basic-lm", ".png")`)
]

---
# Simple linear regression

.pull-left[
![](`r knitr::fig_chunk("basic-lm", ".png")`)
]
.pull-right[
The line added to this scatterplot is the *line of best fit*.

A line like this can be described by two parameters - the *intercept* and the *slope*.

The *intercept* is where the line crosses the *y-axis*.

The *slope* is the *steepness* of the line.

Given these parameters, we can predict the value of **y** - the dependent variable -  at each value of **x** - the independent, predictor variable - using the following formula:

$y = a + bX$
]

---
class: inverse, middle, center
# Multiple linear regression

---
# Predicting children's reading ability

.pull-left[
```{r message = FALSE}
child_data <- read_csv("data/child_data.csv")
head(child_data)
```
]
.pull-right[
.large[
This data is from a study investigating whether children's reading ability can be predicted from their working memory capacity.
]
]

---
# Predicting children's reading ability

.pull-left[
As a starting point, we look at a plot of the relationship between memory span and reading ability, which suggests a positive relationship between the two variables. 
```{r membyread, fig.show = "hide"}
ggplot(child_data, 
       aes(x = mem_span, 
           y = read_ab)) +
  geom_point() + 
  stat_smooth(method = "lm")
```
]
.pull-right[
![](`r knitr::fig_chunk("membyread", ".png")`)
]

---
# Simple linear regression

```{r message = FALSE, highlight.output = c(12, 18)}
simple_model <- lm(read_ab ~ mem_span, data = child_data)
summary(simple_model)
```


---
# Simple vs multiple regression

.pull-left[
```{r echo = FALSE, fig.height = 4, width = 4}
dagify(read_ab ~ mem_span,
       labels = c("read_ab" = "Reading ability",
                  "mem_span" = "Memory span")) %>%
  ggplot(aes(x = x, y = y,
             xend = xend,
             yend= yend)) +
  geom_dag_point(size = 20) +
  geom_dag_edges(arrow_directed = grid::arrow(length = grid::unit(14, "pt"), type = "closed")) +
  geom_dag_label_repel(aes(label = label),
                       label.size = 1,
                       size = 5) +
  #ggdag(use_labels = "label", text = FALSE) +
  theme_void()
```
]

--

.pull-right[
```{r echo = FALSE, fig.height = 4, width = 4}
dagify(read_ab ~ mem_span + IQ + age,
       labels = c("read_ab" = "Reading ability",
                  "mem_span" = "Memory span",
                  "IQ" = "IQ",
                  "age" = "Age")) %>%
  ggplot(aes(x = x, y = y,
             xend = xend,
             yend= yend)) +
  geom_dag_point(size = 18) +
  geom_dag_edges(arrow_directed = grid::arrow(length = grid::unit(10, "pt"), type = "closed")) +
  geom_dag_label_repel(aes(label = label),
                       label.size = 1,
                       size = 5) +
  #ggdag(use_labels = "label", text = FALSE) +
  theme_void()
  
  # ggdag(use_labels = "label",
  #       text = FALSE) +
  # theme_void()
```
]

---
# Simple vs multiple regression

.pull-left[
```{r echo = FALSE}
circles <- data.frame(
    x0 = c(.5, 0),
    y0 = c(0.5, 0),
    r = c(0.5, 0.5),
    label = c("IV", "DV")
)
ggplot(circles) +
    geom_circle(aes(x0 = x0, y0 = y0,
                    r = r, fill = r),
                alpha = 0.5) +
  geom_text(aes(x = x0, y = y0, label = label))+
  coord_equal() +
  theme_void() +
  theme(legend.position = "none")
```
]

--

.pull-right[
```{r echo = FALSE}
circles <- data.frame(
    x0 = c(.5, 0, -.25, -.3),
    y0 = c(0.5, 0, .65, .25),
    r = c(0.5, 0.5, 0.5, .5),
    label = c("IV", "DV", "IV", "IV")
)
ggplot(circles) +
    geom_circle(aes(x0 = x0, y0 = y0,
                    r = r, fill = r),
                alpha = 0.5) +
  geom_text(aes(x = x0, y = y0, label = label))+
  coord_equal() + theme_void() + theme(legend.position = "none")
```
]


---
# Expanding our equation 

.pull-left[
Multiple linear regression deals with multiple predictors.

The *bX* in our regression equation - $y = a + bX + \varepsilon$ - can be expanded. For example, with two predictors, our equation would be:

$y = a_0 + b_1 \times X_1 + b_2 \times X_2 + \varepsilon$

```{r multi-reg}
a <- 2    # Our intercept term
b1 <- 0.65  # Our first regression coefficient
X1 <- rnorm(1000, 6, 1) # Our first predictor 
b2 <- -0.8 # Our second regression coefficient
X2 <- rnorm(1000, 3, 1) # Our second predictor
err <- rnorm(1000, 0, 1) # Our error term
y <- a + b1 * X1 +  b2 * X2 + err # Our response variable
```
]

.pull-right[
```{r echo = FALSE, fig.height = 5}
ggplot(gather(data.frame(y = y, X1 = X1, X2 = X2), predictor, value, -y),
       aes(x = value, y = y)) +
  geom_point() +
  facet_wrap(~predictor) +
  stat_smooth(method= "lm", se = FALSE) +
  labs(x = "predictor")
```
]

---
# Multiple linear regression

```{r highlight.output = c(12,13,14)}
full_model <- lm(read_ab ~ mem_span + age + IQ, data = child_data)
summary(full_model)
```

---
# Is the more complex model better?

We can explicitly compare models using the **anova()** function.

```{r highlight.output = 7}
anova(simple_model, full_model)
```

---
# Comparing regression models

An alternative way to compare models is using the Akaike Information Criterion. 

```{r}
AIC(simple_model)
AIC(full_model)
```

AIC **penalizes** model complexity. A complex model that explains as much variance as a simple model is considered *worse*.

Lower is better!

---
# Comparing predictors

.large[The relative size of a predictor's effect can't *(always)* be judged from their coefficients, since the variables can be on different *scales*.

```{r}
full_model$coefficients
```

```{r echo = FALSE}
child_data %>%
  dplyr::select(-participant)%>%
  gather(variable, value) %>%
  group_by(variable) %>%
  summarise(min = min(value), 
            max = max(value),
            sd = sd(value))
```

]


---
# Standardizing variables

We can standardize our variables to put them on the same scale.

The **scale()** function *mean-centres* and *scales* variables: it subtracts the mean and divides by the standard deviation.

```{r scale_pic, echo = FALSE, fig.height = 4}
child_data %>%
  mutate(IQ_scaled = scale(IQ)) %>%
  dplyr::select(IQ, IQ_scaled) %>%
  gather(variable, IQ_score) %>%
ggplot(aes(x = IQ_score)) +
  geom_density() +
  facet_wrap(~variable,
             scales = "free") +
  theme_bw(base_size = 20)
```

---

```{r}
summary(lm(read_ab ~ mem_span + age + IQ,
           data = mutate_all(child_data, scale)))
```

---
# Standardized coefficients

These coefficients now represent the change in the dependent variable from a **1 standard deviation** change from the **independent variable's mean**.

Standardized coefficients are often called $\beta$ (**beta**) values.

```{r message = FALSE}
library(QuantPsyc)
lm.beta(full_model)
```

---
# Standardized coefficients

sjPlot's `tab_model()` function can also do this for us:

```{r}
tab_model(full_model, show.std = TRUE)
```

---
class: inverse, middle, center
# How many predictors

---
# Which predictors should be in your model?
  
If we look back at the Fear of Crime dataset, there are many potential predictors you could include.

```{r}
head(crime)
```

How do we decide which are important and which to include?
  
---
# What predictors should be in your model?

There are several different common methods.

|Method| Meaning|
|------|-----|
|Hierarchical regression| Variables entered in the order of their known or theoretical importance; known variables are added first, then additional predictors are added and the model fits compared to see which predictors improve model fit.|
|Forced entry| All predictors are entered at once. |
|Stepwise| Predictors are added (forwards, starting with no predictors) or removed (backwards, starting with all predictors) sequentially. Can be performed using **step()**. But please don't. Use *backwards* if you must.|

(see Discovering Statistics using R, section 7.6.4, pages 263-266)


---
class: inverse, middle, center
# Linear regression assumptions

---
## Assumptions of linear regression

Like the t-test and other parametric statistical procedures, linear regression has assumptions.

|Assumption| Description | Comment|
|----------|------|------|
|Independence| Each datapoint should be independent from the others |No repeated measures (for those, you need linear mixed models...)|
|Normally distributed errors| The residuals should be approximately normally distributed around zero. Note that this is often confused with the need for the data to be normally distributed, but it's what's left over from the model that's important! | Best assessed using plots (e.g. **plot()**) |
|Homoscedasticity| The variance at each level of the predictor should be approximately the same (i.e. the residuals should be spread around zero by the same amount) |Best assessed using plots (e.g. **plot()**)|
|Linearity| The relationship between the outcome variable and the predictors should be approximately linear| Use *polynomial* predictors - check the **poly()** function|

See Discovering Statistics Using R, section 7.7.2.1 for more details.

---
# Checking assumptions

```{r fig.height = 3, fig.width = 5}
plot(full_model)
```

---
# Normality of residuals

```{r fig.height = 5}
diag_plots <- plot_model(full_model, type = "diag")
diag_plots[[2]]
```

---
# Normality of residuals

```{r fig.height = 5}
diag_plots[[3]]
```

---
# Homoscedasticity

```{r fig.height = 5}
diag_plots[[4]]
```

---
# Multicollinearity

A potential issue with multiple predictors is that they may be correlated with each other.

Collinearity is a correlation between two predictors; multicollinearity is correlation between *two or more* predictors.

Multicollinearity makes it harder to evaluate the individual contribution of a predictor to a model: it increases the estimated variability of correlated predictors.

```{r message = FALSE}
cor(child_data[, c("IQ", "age", "mem_span")])
```

---
# Multicollinearity

.pull-left[
```{r multico, fig.show = "hide", message = FALSE, fig.height = 5}
diag_plots[[1]]
library(car)
vif(full_model)
```

[Graham, 2003. CONFRONTING MULTICOLLINEARITY IN ECOLOGICAL MULTIPLE REGRESSION](https://esajournals.onlinelibrary.wiley.com/doi/full/10.1890/02-3114)

]
.pull-right[
![](`r knitr::fig_chunk("multico", ".png")`)
]

---
# Reporting results

```{r}
tab_model(full_model, show.std = TRUE)
```

Tables can be particularly useful with multiple regression - especially with a lot of predictors. 

---
class: inverse, middle, center
# Comparing multiple means with categorical predictors

---
# Comparing the means of two groups

Previously, we saw how to use **t.test()** to compare the means of two groups.

```{r}
t.test(FoC ~ sex, data = crime, var.equal = TRUE)
```

---
# Comparing three or more means with ANOVA

The *t.test()* can only handle two groups. 

When we have three or more groups, we need to use a One-Way Analysis of Variance (ANOVA).

```{r hypothetical-norms, echo = FALSE, fig.height = 5}
tibble(iq = 0:200,
       group_a = dnorm(iq, mean = 100, sd = 15),
       group_b = dnorm(iq, mean = 110, sd = 15),
       group_c = dnorm(iq, mean = 70, sd = 15)) %>%
  gather(group, density, -iq) %>%
  ggplot(aes(x = iq, y = density)) +
  geom_line(aes(group = group, linetype = group)) +
  theme_classic() 
```

---
# How does ANOVA work?

With a t-test, we typically ask the question "Is the difference between these two means significantly different from zero?"  

$$\mu^1 \neq \mu^2$$

With an ANOVA, we ask the question "Are any of these means different from each other?"

$$\mu^1 \neq \mu^2 \neq \mu^3 ...$$

Another way to phrase this is "Do any of these means differ from the *grand* mean?"

---
# The (grand) mean and the variance

The *grand* mean is the mean across all conditions. 

## A worked example
.pull-left[
A researcher wants to examine the effect of noisy environments on test performance. 
She recruits 150 participants and splits them into three groups. 

One group performs the test without any environmental noise. A second group performs the test with fairly quiet noise. A third group performs the test with loud noise. The dependent variable is their score (out of 10) on the test.

]

.pull-right[
```{r sim-dat}
noise_test <- 
  gather(tibble(none = rnorm(50, 8, 1),
                quiet = rnorm(50, 7, 1),
                loud = rnorm(50, 5, 1)),
         noise, test_score) %>%
  mutate(participant = 1:150)
```
]

---
# How the data is structured

.pull-left[
```{r}
noise_test
```
]
.pull-right[
One column per variable!

One column - *noise* - is the categorical predictor variable that tells which group each participant was in.

One column - *test_score* - is the dependent variable.

The final column - participant - is a (unique - each participant always has the same identifier) participant identifier.
]

---
# The mean as a model (again)

We went through this in detail last time, but here's how it applies here.

The simplest model of this data is to use the grand mean across all conditions.

.pull-left[
```{r echo = FALSE,fig.height=5}
ggplot(noise_test,
       aes(x = participant,
           y = test_score,
           colour = noise)) +
  geom_point() + 
  geom_hline(aes(yintercept = mean(test_score))) + 
  theme_classic(base_size = 20)
```
]

.pull-right[
The grand mean test score is `r round(mean(noise_test$test_score), 2)`, shown by the black line.

The total variability in our data is the sum of the squared differences from the grand mean - the Total Sum of Squares, $SS_t$.

]

---
# The group means as a model 

The model we're interested in is the means as a function of group.

.pull-left[
```{r echo = FALSE,fig.height=5, warning = FALSE}
noise_test <- 
  noise_test %>%
  group_by(noise) %>%
  mutate(group_mean = mean(test_score))
ggplot(noise_test, 
       aes(x = participant,
           y = test_score,
           colour = noise)) +
  geom_point(size = 1) + 
  geom_hline(aes(yintercept = mean(test_score))) + 
  #geom_segment(aes(x = 1:50, y = mean(test_score[1:50])), colour = "green") + 
  stat_summary(fun = "mean", geom = "crossbar")+
  geom_segment(data = noise_test[1:50, ],
               aes(x = 1, xend = 50, y = group_mean, yend = group_mean),
               size = 2) +
  geom_segment(data = noise_test[51:100, ],
               aes(x = 51, xend = 100, y = group_mean, yend = group_mean),
               size = 2) +
  geom_segment(data = noise_test[101:150, ],
               aes(x = 101, xend = 150, y = group_mean, yend = group_mean),
               size = 2) +
  theme_classic(base_size = 20)
```
]

.pull-right[
Our Model Sum of Squares - $SS_m$ - is the sum of the squared differences of each group's mean from the *grand mean*.

The group means are shown here using coloured lines.

The final quantity, the Residual Sum of Squares - $SS_r$ is the sum of the squared differences of each individual observation from the mean of the group to which it belongs.

]

---
# Degrees of freedom

.large[
We now have measures of the total amount of variability explained by the data, the total amount explained by our model, and the amount left over by our model.

However, these numbers are biased because different amounts of values went into their calculation - 3 were used to calculate the $SS_m$, while many more were used to calculate $SS_t$ and $SS_r$. 

We correct these using the *degrees of freedom*. Specifically, we need to correct $SS_r$ and $SS_m$ with the residual degrees of freedom - $df_r$ and the model degrees of freedom - $df_m$.
]

---
# Degrees of freedom

.large[
The model degrees of freedom is simply the number of groups - 1; where *k* = number of groups:

$$df_m = k - 1$$

The residual degrees of freedom is the sum of all the degrees of freedom for each group.

$$df_r = \sum{df_{group^k}}$$
]

---
# Mean squared error and the F-ratio

Finally, we divide our sums of squares - $SS_m$ and $SS_r$ by $df_m$ and $df_r$ respectively, giving us the mean squared error of the model - $MS_m$ - and mean squared error of the residuals - $MS_r$.

$$MS_m = \frac{SS_m}{df_m}$$

$$MS_r = \frac{SS_r}{df_r}$$

The ratio of these two quantities is the *F-ratio*.

$$F = \frac{MS_m}{MS_r}$$

In English, the F-ratio is the ratio of the variability explained by the model to variability unexplained by the model. So, higher is better.

---
class: inverse, center, middle
# How to run a one-way between subjects ANOVA

---
# How to run ANOVA with the *afex* package

Although the standard R function for ANOVA, **aov()**, works, it can be fiddly to use.

The **afex** package provides several easier methods for running ANOVA.

We'll use the **aov_ez()** function.

```{r}
noise_aov <- aov_ez(dv = "test_score",
                    between = "noise",
                    id = "participant",
                    data = noise_test)
```


---
# Checking the results

```{r highlight.output = 5}
noise_aov
```

There's a highly significant effect of the factor *noise*.

But ANOVA only tells us that there is a difference; not what the difference is!

---
# Follow-up contrasts

We can use the **emmeans** package to get more information about our results.

First, let's run the **emmeans()** function to get the means for each condition.

```{r}
means_noise <- emmeans(noise_aov, ~noise)
means_noise
```

It looks like performance was best is when there was no noise, with the worst performance when there was loud noise.

---
# Follow-up contrasts

After calculating the means, we can then compare all of the means to each other using the **pairs()** function.

```{r}
pairs(means_noise)
```

Note that this corrects the p-values for multiple comparisons. There are three possible comparisons, each with a significance threshold of p = .05; the more possible comparisons, the more you have to correct for false positives.

---
# Visualizing the results

As ever, it's best to support your inferences with visualizations. 

**afex_plot()** from the **afex** package can automatically create plots from the fitted ANOVA.

```{r fig.height = 5}
afex_plot(noise_aov, x = "noise") + theme_classic()
```

---
# Assumptions of ANOVA

Just like the t-test and our linear regressions, homogeneity of variance is assumed.

The data should be normally distributed for each group.

```{r fig.height=5}
ggplot(noise_test, aes(x = test_score)) + geom_histogram(bins = 9) + facet_wrap(~noise)
```

---
# Assumptions of ANOVA

This can be explicitly tested using the **leveneTest()** function from the **car** package.

```{r}
library(car)
leveneTest(test_score ~ noise, data = noise_test)
```


---
# Assumptions of (between-subjects) ANOVA

Each observation should be independent - i.e. there should be no repeated measures. 

Each participant is in one group and one group only, and contributes one data point to that group.



---
# Next week

Repeated-measures ANOVA.

Factorial and mixed ANOVA.

These are covered in chapters 12-14 of Discovering Statistics Using R.

---
class: title-slide-final, middle, inverse
background-image: url('images/University of Lincoln_logo_General White Landscape.png')
background-size: 500px
background-position: 50% 10%

