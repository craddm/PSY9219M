---
title: "Multiple predictors and multiple means"
subtitle: "PSY9219M - Research Methods and Skills"
author: "Dr Matt Craddock"
date: "20/11/2018"
output:
  xaringan::moon_reader:
    css: ["default", "default-fonts", "css/my-theme.css"]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
      beforeInit: "js/macros.js"
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
library(tidyverse)
library(afex)
library(emmeans)
crime <- read_csv("data/crime.csv")
```

# Different correlations

```{r corr-dir, fig.width = 12, fig.height = 6, echo = FALSE}
pos_corr <- matrix(0.9, 2, 2)
diag(pos_corr) <- 1
neg_corr <- matrix(-.9, 2, 2)
diag(neg_corr) <- 1
no_corr <- matrix(0, 2, 2)
diag(no_corr) <- 1
corr_examps <- data.frame(rbind(MASS::mvrnorm(200, rep(0, 2), Sigma = pos_corr),
                          MASS::mvrnorm(200, rep(0, 2), Sigma = neg_corr),
                          MASS::mvrnorm(200, rep(0, 2), Sigma = no_corr)),
                          direction = rep(c("positive", "negative", "none"),
                                          each = 200))
names(corr_examps) <- c("X", "Y", "direction")
ggplot(corr_examps, aes(x = X, y = Y)) + 
  geom_point() +
  facet_wrap(~direction) +
  theme_classic() +
  theme(text = element_text(size = 26))
```

---
# Correlation summary

.large[
Correlation coefficients are bound in a range from -1 to 1.

Negative coefficients mean that as one variable increases, the other decreases.

Positive coefficients mean that as one variable increases, the other also increases.
]

```{r corr-dir-again, fig.width = 8, fig.height = 4, echo = FALSE}
pos_corr <- matrix(0.9, 2, 2)
diag(pos_corr) <- 1
neg_corr <- matrix(-.9, 2, 2)
diag(neg_corr) <- 1
no_corr <- matrix(0, 2, 2)
diag(no_corr) <- 1
corr_examps <- data.frame(rbind(MASS::mvrnorm(200, rep(0, 2), Sigma = pos_corr),
                          MASS::mvrnorm(200, rep(0, 2), Sigma = neg_corr),
                          MASS::mvrnorm(200, rep(0, 2), Sigma = no_corr)),
                          direction = rep(c("positive", "negative", "none"),
                                          each = 200))
names(corr_examps) <- c("X", "Y", "direction")
ggplot(corr_examps, aes(x = X, y = Y)) + 
  geom_point() +
  facet_wrap(~direction) +
  theme_classic() +
  theme(text = element_text(size = 16))
```

---
#Correlation, regression and prediction

.pull-left[
Correlation quantifies the *strength* and *direction* of an association between two continuous variables.

But what if we want to *predict* the values of one variable from those of another?

For example, as Emotionality increases, *how much* does Fear of Crime change?

```{r basic-lm, fig.show = "hide", fig.height=5}
ggplot(crime, 
       aes(x = E, y = FoC)) +
  geom_point() +
  stat_smooth(method = "lm", se = FALSE) +
  theme_classic() +
  labs(x = "Emotionality",
       y = "Fear of crime")
```
]
.pull-right[
![](`r knitr::fig_chunk("basic-lm", ".png")`)
]

---
# Simple linear regression

.pull-left[
![](`r knitr::fig_chunk("basic-lm", ".png")`)
]
.pull-right[
The line added to this scatterplot is the *line of best fit*.

A line like this can be described by two parameters - the *intercept* and the *slope*.

The *intercept* is where the line crosses the *y-axis*.

The *slope* is the *steepness* of the line.

Given these parameters, we can predict the value of **y** - the dependent variable -  at each value of **x** - the independent, predictor variable - using the following formula:

$y = a + bX$
]

---
class: inverse, middle, center
# Multiple linear regression

---
# More about regression equations

.pull-left[
As noted earlier, the basic linear regression equation is $y = a + bX$.

We can use this to create a dataset with the specified relationship!

```{r create-dat}
a <- 1    # Our intercept term
b <- 0.5  # Our regression coefficient
X <- rnorm(1000, 6, 1) # Our predictor - a normally distributed variable
y <- a + b * X  # Our response variable
```
]
.pull-right[
```{r fig.height = 5, echo = FALSE}
cowplot::plot_grid(qplot(X, bins = 9), qplot(y, bins = 9))
```
]

---
# More about regression equations

.pull-left[
If we fit a linear model using **lm()**, we get exactly our pre-specified intercept and coefficient out. The **coef()** function extracts the model coefficients.

```{r}
coef(lm(y ~ X))
```

These are exactly what we specified above!
]

.pull-right[
```{r fig.height = 5, echo = FALSE}
qplot(X, y)
```
]
---
# More about regression equations

.pull-left[
Remember: the slope describes the steepness of the line.

Remember that the intercept describes where the regression line crosses the y-axis **when the value of the predictor is zero**.

Let's expand our plot out so that we can see the relevant region, and draw a line with *geom_abline()* using our regression coefficients.

```{r reg-line, fig.height = 5, fig.width = 5, fig.show = "hide", message = FALSE, warning = FALSE}
qplot(X, y) +
  geom_abline(slope = 0.5, intercept = 1) +
  lims(x = c(0, 9), y = c(1, 5))
```
]
.pull-right[
![](`r knitr::fig_chunk("reg-line", "png")`)
]
---
# More about regression equations

.pull-left[
However, this all looks a little too perfect. Real data is rarely so accurately measured!

We missed an additional regression term - the *error* term, or $\varepsilon$ - which we should add to the end of our basic linear regression equation:

$y = a + bX + \varepsilon$.

```{r x-noise, fig.height = 5, fig.show = "hide"}
a <- 1    # Our intercept term
b <- 0.5  # Our regression coefficient
X <- rnorm(1000, 6, 1) # Our predictor - a normally distributed variable
y <- a + b * X  + rnorm(1000, 0, 1) # Our response variable
qplot(X, y)
```

]

.pull-right[
![](`r knitr::fig_chunk("x-noise", "png")`)
]

---
# More about regression equations

.pull-left[

If we extract the coefficients for a model fit to this data, with the added noise...

```{r noise-lm}
coef(lm(y ~ X))
```

... we still get more or less the same coefficients as we specified earlier, but now not quite so precise.

This is a much more realistic situation!

]

.pull-right[
![](`r knitr::fig_chunk("x-noise", "png")`)
]

---
class: inverse, middle, center
# Multiple linear regression

---
# Simple linear regression

Let's take a quick look at the summary for the linear regression of our simulated data. 

```{r echo = FALSE}
summary(lm(y ~ X))
```

---
# Simple linear regression

The R^2 measure of model fit is `r round(summary(lm(y ~ X))$r.squared, 2)`, which leaves `r 100 - round(summary(lm(y ~ X))$r.squared, 2) * 100`% of the variance  unexplained.

Simple linear regression involves prediction of one variable from another. Sometimes that's all you're interested in!

Most of the time, however, that still leaves a lot of variability unexplained.

```{r resid-plot, echo = FALSE, fig.height = 4, fig.width = 6}
cowplot::plot_grid(qplot(X, y) +
                     geom_smooth(method = "lm", se = FALSE),
                   qplot(fitted(lm(y ~ X)), resid(lm(y ~ X))) +
                     geom_smooth(method = "lm", se = FALSE))
```

---
# Expanding our equation

.pull-left[
Multiple linear regression deals with multiple predictors.

The *bX* in our regression equation - $y = a + bX + \varepsilon$ - can be expanded out. For example, with two predictors, our equation would be:

$y = a + b1 \times X1 + b2 \times X2 + \varepsilon$

```{r multi-reg}
a <- 2    # Our intercept term
b1 <- 0.65  # Our first regression coefficient
X1 <- rnorm(1000, 6, 1) # Our first predictor 
b2 <- -0.8 # Our second regression coefficient
X2 <- rnorm(1000, 3, 1) # Our second predictor
err <- rnorm(1000, 0, 1) # Our error term
y <- a + b1 * X1 +  b2 * X2 + err # Our response variable
```
]

.pull-right[
```{r echo = FALSE, fig.height = 5}
ggplot(gather(data.frame(y = y, X1 = X1, X2 = X2), predictor, value, -y),
       aes(x = value, y = y)) +
  geom_point() +
  facet_wrap(~predictor) +
  stat_smooth(method= "lm", se = FALSE) + labs(x = "predictor")
```
]

---
# Multiple linear regression

.pull-left[
We can begin regressing just one predictor at a time.
```{r multi-reg-two, eval = FALSE}
a <- 2    # Our intercept term
b1 <- 0.65  # Our first regression coefficient
X1 <- rnorm(1000, 6, 1) # Our first predictor 
b2 <- -0.8 # Our second regression coefficient
X2 <- rnorm(1000, 3, 1) # Our second predictor
err <- rnorm(1000, 0, 1) # Our error term
y <- a + b1 * X1 +  b2 * X2 + err # Our response variable
```
]
.pull-right[
```{r}
coef(lm(y ~ X1))
```

```{r}
coef(lm(y ~ X2))
```
]

---
# Multiple linear regression

.pull-left[
Each of these simple models predicts a reasonable amount of the variance in *y*.

```{r multi-reg-three, eval = FALSE}
a <- 2    # Our intercept term
b1 <- 0.65  # Our first regression coefficient
X1 <- rnorm(1000, 6, 1) # Our first predictor 
b2 <- -0.8 # Our second regression coefficient
X2 <- rnorm(1000, 3, 1) # Our second predictor
err <- rnorm(1000, 0, 1) # Our error term
y <- a + b1 * X1 + b2 * X2 + err # Our response variable
```
]

.pull-right[
```{r}
summary(lm(y ~ X1))$r.squared
summary(lm(y ~ X2))$r.squared
```
]

---
# Multiple linear regression

.pull-left[
Can we do better with a model that includes both predictors?
```{r multi-reg-four, eval = FALSE}
a <- 2    # Our intercept term
b1 <- 0.65  # Our first regression coefficient
X1 <- rnorm(1000, 6, 1) # Our first predictor 
b2 <- -0.8 # Our second regression coefficient
X2 <- rnorm(1000, 3, 1) # Our second predictor
err <- rnorm(1000, 0, 1) # Our error term
y <- a + b1 * X1 + b2 * X2 + err # Our response variable
```
]
.pull-right[
Adding predictors is easy - we use the **+** symbol!
```{r}
lm(y ~ X1 + X2)
```
]

---
# Multiple linear regression

.pull-left[
```{r}
summary(lm(y ~ X1 + X2))
```
]

---
# Multiple linear regression

So does our multiple linear model explain more of the variance than either of our simple linear models?
.pull-left[

```{r}
summary(lm(y ~ X1))$r.squared
summary(lm(y ~ X2))$r.squared
summary(lm(y ~ X1 + X2))$r.squared
```
]
---
# Comparing regression models using ANOVA

We can explicitly compare models using the **anova()** function.

```{r}
anova(lm(y ~ X1), lm(y ~ X1 + X2))
```

NOTE: this is Analysis of Variance (ANOVA), but, confusingly, you don't use this function to run the kind of ANOVA we'll be doing later!

---
# How many predictors?

If we look back at the Fear of Crime dataset, there are many potential predictors you could include.

```{r}
head(crime)
```

How do we decide which are important and which to include?

---
# How many predictors?

There are several different common methods.

|Method| Meaning|
|------|-----|
|Hierarchical regression| Variables entered in the order of their known or theoeretical importance; known variables are added first, then additional predictors are added and the model fits compared to see which predictors improve model fit.|
|Forced entry| All predictors are entered at once. |
|Stepwise| Predictors are added (forwards, starting with no predictors) or removed (backwards, starting with all predictors) sequentially. Can be performed using **step()**. But please don't. Use *backwards* if you must.|

(see Discovering Statistics using R, section 7.6.4, pages 263-266)

---
## Assumptions of linear regression

Like the t-test and other parametric statistical procedures, linear regression has assumptions.

|Assumption| Description | Comment|
|----------|------|------|
|Independence| Each datapoint should be independent from the others |No repeated measures (for those, you need linear mixed models...)|
|Normally distributed errors| The residuals should be approximately normally distributed around zero. Note that this is often confused with the need for the data to be normally distributed, but it's what's left over from the model that's important! | Best assessed using plots (e.g. **plot()**) |
|Homoscedascity| The variance at each level of the predictor should be approximately the same (i.e. the residuals should be spread around zero by the same amount) |Best assessed using plots (e.g. **plot()**)|
|Linearity| The relationship between the outcome variable and the predictors should be approximately linear| Use *polynomial* predictors - check the **poly()** function|

See Discovering Statistics Using R, section 7.7.2.1 for more details.

---

```{r fig.height = 3.5, fig.width = 5}
plot(lm(y ~ X1 + X2))
```

---
class: inverse, middle, center
# Comparing multiple means with categorical predictors

---
# Comparing the means of two groups

Previously, we saw how to use **t.test()** to compare the means of two groups.

```{r}
t.test(FoC ~ sex, data = crime, var.equal = TRUE)
```

---
# Comparing three or more means with ANOVA

The *t.test()* can only handle two groups. 

When we have three or more groups, we need to use a One-Way Analysis of Variance (ANOVA).

```{r hypothetical-norms, echo = FALSE, fig.height = 5}
tibble(iq = 0:200,
       group_a = dnorm(iq, mean = 100, sd = 15),
       group_b = dnorm(iq, mean = 110, sd = 15),
       group_c = dnorm(iq, mean = 70, sd = 15)) %>%
  gather(group, density, -iq) %>%
  ggplot(aes(x = iq, y = density)) +
  geom_line(aes(group = group, linetype = group)) +
  theme_classic() 
```

---
# How does ANOVA work?

With a t-test, we typically ask the question "Is the difference between these two means significantly different from zero?"  

$$\mu^1 \neq \mu^2$$

With an ANOVA, we ask the question "Are any of these means different from each other?"

$$\mu^1 \neq \mu^2 \neq \mu^3 ...$$

Another way to phrase this is "Do any of these means differ from the *grand* mean?"

---
# The (grand) mean and the variance

The *grand* mean is the mean across all conditions. 

## A worked example
.pull-left[
A researcher wants to examine the effect of noisy environments on test performance. 
She recruits 150 participants and splits them into three groups. 

One group performs the test without any environmental noise. A second group performs the test with fairly quiet noise. A third group performs the test with loud noise. The dependent variable is their score (out of 10) on the test.

]

.pull-right[
```{r sim-dat}
noise_test <- 
  gather(tibble(none = rnorm(50, 8, 1),
                quiet = rnorm(50, 7, 1),
                loud = rnorm(50, 5, 1)),
         noise, test_score) %>%
  mutate(participant = 1:150)
```
]

---
# How the data is structured

.pull-left[
```{r}
noise_test
```
]
.pull-right[
One column per variable!

One column - *noise* - is the categorical predictor variable that tells which group each participant was in.

One column - *test_score* - is the dependent variable.

The final column - participant - is a (unique - each participant always has the same identifier) participant identifier.
]

---
# The mean as a model (again)

We went through this in detail last time, but here's how it applies here.

The simplest model of this data is to use the grand mean across all conditions.

.pull-left[
```{r echo = FALSE,fig.height=5}
ggplot(noise_test,
       aes(x = participant,
           y = test_score,
           colour = noise)) +
  geom_point() + 
  geom_hline(aes(yintercept = mean(test_score))) + 
  theme_classic()
```
]

.pull-right[
The grand mean test score is `r round(mean(noise_test$test_score), 2)`, shown by the black line.

The total variability in our data is the sum of the squared differences from the grand mean - the Total Sum of Squares, $SS_t$.

]

---
# The group means as a model 

The model we're interested in is the means as a function of group.

.pull-left[
```{r echo = FALSE,fig.height=5, warning = FALSE}
noise_test <- 
  noise_test %>%
  group_by(noise) %>%
  mutate(group_mean = mean(test_score))
ggplot(noise_test, 
       aes(x = participant,
           y = test_score,
           colour = noise)) +
  geom_point(size = 1) + 
  geom_hline(aes(yintercept = mean(test_score))) + 
  #geom_segment(aes(x = 1:50, y = mean(test_score[1:50])), colour = "green") + 
  stat_summary(fun.y = "mean", geom = "crossbar")+
  geom_segment(data = noise_test[1:50, ],
               aes(x = 1, xend = 50, y = group_mean, yend = group_mean),
               size = 2) +
  geom_segment(data = noise_test[51:100, ],
               aes(x = 51, xend = 100, y = group_mean, yend = group_mean),
               size = 2) +
  geom_segment(data = noise_test[101:150, ],
               aes(x = 101, xend = 150, y = group_mean, yend = group_mean),
               size = 2) +
  theme_classic()
```
]

.pull-right[
Our Model Sum of Squares - $SS_m$ - is the sum of the squared differences of each group's mean from the *grand mean*.

The group means are shown here using coloured lines.

The final quantity, the Residual Sum of Squares - $SS_r$ is the sum of the squared differences of each individual observation from the mean of the group to which it belongs.

]

---
# Degrees of freedom

.large[
We now have measures of the total amount of variability explained by the data, the total amount explained by our model, and the amount left over by our model.

However, these numbers are biased because different amounts of values went into their calculation - 3 were used to calculate the $SS_m$, while many more were used to calculate $SS_t$ and $SS_r$. 

We correct these using the *degrees of freedom*. Specifically, we need to correct $SS_r$ and $SS_m$ with the residual degrees of freedom - $df_r$ and the model degrees of freedom - $df_m$.
]

---
# Degrees of freedom

.large[
The model degrees of freedom is simply the number of groups - 1; where *k* = number of groups:

$$df_m = k - 1$$

The residual degrees of freedom is the sum of all the degrees of freedom for each group.

$$df_r = \sum{df_{group^k}}$$
]

---
# Mean squared error and the F-ratio

Finally, we divide our sums of squares - $SS_m$ and $SS_r$ by $df_m$ and $df_r$ respectively, giving us the mean squared error of the model - $MS_m$ - and mean squared error of the residuals - $MS_r$.

$$MS_m = \frac{SS_m}{df_m}$$

$$MS_r = \frac{SS_r}{df_r}$$

The ratio of these two quantities is the *F-ratio*.

$$F = \frac{MS_m}{MS_r}$$

In English, the F-ratio is the ratio of the variability explained by the model to variability unexplained by the model. So, higher is better.

---
class: inverse, center, middle
# How to run a one-way between subjects ANOVA

---
# How to run ANOVA with the *afex* package

Although the standard R function for ANOVA, **aov()**, works, it can be fiddly to use.

The **afex** package provides several easier methods for running ANOVA.

We'll use the **aov_ez()** function.

```{r}
noise_aov <- aov_ez(dv = "test_score",
                    between = "noise",
                    id = "participant",
                    data = noise_test)
```


---
# Checking the results

```{r}
noise_aov
```

There's a highly significant effect of the factor *noise*.

But ANOVA only tells us that there is a difference; not what the difference is!

---
# Follow-up contrasts

We can use the **emmeans** package to get more information about our results.

First, let's run the **emmeans()** function to get the means for each condition.

```{r}
means_noise <- emmeans(noise_aov, ~noise)
means_noise
```

It looks like performance was best is when there was no noise, with the worst performance when there was loud noise.

---
# Follow-up contrasts

After calculating the means, we can then compare all of the means to each other using the **pairs()** function.

```{r}
pairs(means_noise)
```

Note that this corrects the p-values for multiple comparisons. There are three possible comparisons, each with a significance threshold of p = .05; the more possible comparisons, the more you have to correct for false positives.

---
# Visualizing the results

As ever, it's best to support your inferences with visualizations. 

**afex_plot()** from the **afex** package can automatically create plots from the fitted ANOVA.

```{r fig.height = 5}
afex_plot(noise_aov, x = "noise") + theme_classic()
```

---
# Assumptions of ANOVA

Just like the t-test and our linear regressions, homogeneity of variance is assumed.

This can be explicitly tested using the **leveneTest()** function from the **car** package.

```{r}
library(car)
leveneTest(test_score ~ noise, data = noise_test)
```

---
# Assumptions of ANOVA

The data should be normally distributed for each group.

```{r fig.height=5}
ggplot(noise_test, aes(x = test_score)) + geom_histogram(bins = 9) + facet_wrap(~noise)
```

---
# Assumptions of (between-subjects) ANOVA

Each observation should be independent - i.e. there should be no repeated measures. 

Each participant is in one group and one group only, and contributes one data point to that group.



---
# Next week

Repeated-measures ANOVA.

Factorial and mixed ANOVA.

These are covered in chapters 12-14 of Discovering Statistics Using R.

---
class: title-slide-final, middle, inverse
background-image: url('images/University of Lincoln_logo_General White Landscape.png')
background-size: 500px
background-position: 50% 10%

---
# T-tests and ANOVAs are linear models

The t-test can actually be considered part of the linear regression family!

```{r}
summary(lm(test_score ~ noise, data = noise_test))
```

---
# Comparing the t-test and the linear model
.pull-left[
### t-test
```{r}
t.test(FoC ~ sex, data = crime,
       var.equal = TRUE)$estimate
t.test(FoC ~ sex, data = crime,
       var.equal = TRUE)$p.value
```
]
.pull-right[
### Linear model
```{r}
coef(lm(FoC ~ sex, data = crime))
summary(lm(FoC ~sex, data = crime))$coefficients[2, 4]
```
]

---
# The linear model with a two-group categorical predictor

When using categorical predictors, the linear model has to convert the categories to numerical values. 

With two groups, this is simple: one group is coded as zero, the other as one! Note: this is called *dummy* or *treatment* coding, and is the default behaviour in R.

```{r coef-lm}
coef(lm(FoC ~sex, data = crime))
```

In this case, the *female* group is treated as the zero or *reference* group. The *male* group is specified as the one or *dummy/treatment* group.

Remember, regression coefficients are typically the change in the response variable for a *one unit* increase in the predictor.


---
class: title-slide-final, middle, inverse
background-image: url('images/University of Lincoln_logo_General White Landscape.png')
background-size: 500px
background-position: 50% 10%
